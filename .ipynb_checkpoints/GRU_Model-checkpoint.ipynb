{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IWTL\\anaconda3\\envs\\env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\IWTL\\anaconda3\\envs\\env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\IWTL\\anaconda3\\envs\\env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\IWTL\\anaconda3\\envs\\env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\IWTL\\anaconda3\\envs\\env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\IWTL\\anaconda3\\envs\\env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 9923058490034687398\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 10004617626\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 5157542491728196145\n",
      "physical_device_desc: \"device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# check GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "Path_to_Train = r'.\\data\\processed\\cs_train.txt'\n",
    "Path_to_Test = r'.\\data\\processed\\cs_test.txt'\n",
    "checkpoint_dir = r'.\\checkpoint'\n",
    "\n",
    "layers = 1\n",
    "rnn_size = 100\n",
    "batch_size = 50\n",
    "drop_keep_prob = 0.7\n",
    "\n",
    "n_epochs = 3\n",
    "learning_rate = 0.001\n",
    "decay = 0.96\n",
    "decay_steps = 1e4\n",
    "grad_cap = 0\n",
    "print_step = 1e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data = pd.read_csv(Path_to_Train, sep = '\\t', dtype = {'item_id':np.int64})\n",
    "valid = pd.read_csv(Path_to_Test, sep = '\\t', dtype = {'item_id':np.int64})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>item_type</th>\n",
       "      <th>session</th>\n",
       "      <th>cross_session</th>\n",
       "      <th>item_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1002396</td>\n",
       "      <td>1394626</td>\n",
       "      <td>1.121625e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>2727</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002396</td>\n",
       "      <td>1301654</td>\n",
       "      <td>1.121625e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>2727</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1002396</td>\n",
       "      <td>1394620</td>\n",
       "      <td>1.121626e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>2727</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1005493</td>\n",
       "      <td>1292063</td>\n",
       "      <td>1.121702e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>3059</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1005493</td>\n",
       "      <td>1291559</td>\n",
       "      <td>1.121702e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>3059</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id     timestamp  item_type  session  cross_session  item_idx\n",
       "0  1002396  1394626  1.121625e+09          0     2727              0         0\n",
       "1  1002396  1301654  1.121625e+09          0     2727              0         1\n",
       "2  1002396  1394620  1.121626e+09          1     2727              0         2\n",
       "3  1005493  1292063  1.121702e+09          0     3059              1         3\n",
       "4  1005493  1291559  1.121702e+09          0     3059              1         4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# item_idx\n",
    "itemids = data['item_id'].unique()\n",
    "n_items = len(itemids)\n",
    "itemidmap = pd.Series(data=np.arange(n_items), index=itemids).to_dict()\n",
    "data['item_idx'] = data['item_id'].map(lambda x: itemidmap[x])\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  3,  7, 10, 23])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mini-batch\n",
    "offset_sessions = np.zeros(data['cross_session'].nunique()+1, dtype=np.int32)\n",
    "offset_sessions[1:] = data.groupby('cross_session').size().cumsum()\n",
    "offset_sessions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\IWTL\\anaconda3\\envs\\env\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# placeholder & learning rate\n",
    "X = tf.placeholder(tf.int32, [batch_size], name='input')\n",
    "Y = tf.placeholder(tf.int32, [batch_size], name='output')\n",
    "States = [tf.placeholder(tf.float32, [batch_size, rnn_size], name='rnn_state') for _ in range(layers)]\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "lr = tf.maximum(1e-5,tf.train.exponential_decay(\n",
    "    learning_rate, global_step, decay_steps, decay, staircase=True\n",
    ")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding matrix\n",
    "with tf.variable_scope('gru_layer', reuse=tf.AUTO_REUSE):\n",
    "    initializer = tf.glorot_uniform_initializer()\n",
    "    embedding = tf.get_variable('embedding', [n_items, rnn_size], initializer=initializer)\n",
    "    softmax_W = tf.get_variable('softmax_w', [n_items, rnn_size], initializer=initializer)\n",
    "    softmax_b = tf.get_variable('softmax_b', [n_items], initializer=tf.zeros_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-3161deaffc11>:3: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-9-3161deaffc11>:5: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    }
   ],
   "source": [
    "# gru cell\n",
    "with tf.variable_scope('gru_cell', reuse=tf.AUTO_REUSE):\n",
    "    cell = tf.nn.rnn_cell.GRUCell(rnn_size, activation=tf.nn.tanh)\n",
    "    drop_cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=drop_keep_prob)\n",
    "    stacked_cell = tf.nn.rnn_cell.MultiRNNCell([drop_cell] * layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\IWTL\\anaconda3\\envs\\env\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.nn.embedding_lookup(embedding, X)\n",
    "output, state_ = stacked_cell(inputs, tuple(States))\n",
    "final_state = state_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### for training\n",
    "sampled_W = tf.nn.embedding_lookup(softmax_W, Y)\n",
    "sampled_b = tf.nn.embedding_lookup(softmax_b, Y)\n",
    "logits = tf.matmul(output, sampled_W, transpose_b=True) + sampled_b\n",
    "### cross-entropy loss\n",
    "yhat = tf.nn.softmax(logits)\n",
    "cost = tf.reduce_mean(-tf.log(tf.diag_part(yhat)+1e-24))\n",
    "### for prediction\n",
    "logits_all = tf.matmul(output, softmax_W, transpose_b=True) + softmax_b\n",
    "yhat_all = tf.nn.softmax(logits_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\IWTL\\anaconda3\\envs\\env\\lib\\site-packages\\tensorflow\\python\\ops\\array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "# Adam optimizer.\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "tvars = tf.trainable_variables()\n",
    "gvs = optimizer.compute_gradients(cost, tvars)\n",
    "if grad_cap > 0:\n",
    "    capped_gvs = [(tf.clip_by_norm(grad, grad_cap), var) for grad, var in gvs]\n",
    "else:\n",
    "    capped_gvs = gvs \n",
    "train_op = optimizer.apply_gradients(capped_gvs, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## session start\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(tf.global_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4] ... [45 46 47 48 49]\n",
      "49\n",
      "[ 0  3  7 10 23]\n",
      "[ 3  7 10 23 32]\n"
     ]
    }
   ],
   "source": [
    "iters = np.arange(batch_size)\n",
    "maxiter = iters.max()\n",
    "print(iters[:5], \"...\", iters[-5:])\n",
    "print(maxiter)\n",
    "start = offset_sessions[iters]\n",
    "end = offset_sessions[iters+1]\n",
    "print(start[:5])\n",
    "print(end[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[ 0  3  7 10 23]\n"
     ]
    }
   ],
   "source": [
    "minlen = (end-start).min()\n",
    "out_idx = data.item_idx.values[start]\n",
    "print(minlen)\n",
    "print(out_idx[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  3  7 10 23]\n",
      "[ 1  4  8 11 24]\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "in_idx = out_idx\n",
    "out_idx = data.item_idx.values[start+i+1]\n",
    "print(in_idx[:5])\n",
    "print(out_idx[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3  7 10 23 32]\n",
      "[ 2  5  9 12 25]\n",
      "[0 2 5 6 8]\n"
     ]
    }
   ],
   "source": [
    "start = start+minlen-1\n",
    "mask = np.arange(len(iters))[(end-start)<=1]\n",
    "print(end[:5])\n",
    "print(start[:5])\n",
    "print(mask[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50  1 51  3  4  5  6  7  8  9]\n",
      "[429   5 502  12  25  34  37  40  66  69]\n",
      "[502   7 505  23  32  35  38  64  67  71]\n"
     ]
    }
   ],
   "source": [
    "iters[0] = 50\n",
    "iters[2] = 51\n",
    "print(iters[:10])\n",
    "\n",
    "start[0] = offset_sessions[50]\n",
    "end[0] = offset_sessions[50+1]\n",
    "start[2] = offset_sessions[51]\n",
    "end[2] = offset_sessions[51+1]\n",
    "print(start[:10])\n",
    "print(end[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[429   5 502  12  25 505 514  40 517  69]\n",
      "[502   7 505  23  32 514 517  64 520  71]\n"
     ]
    }
   ],
   "source": [
    "for idx in mask:\n",
    "    maxiter += 1\n",
    "    if maxiter >= len(offset_sessions)-1:\n",
    "        finished = True\n",
    "        break\n",
    "    iters[idx] = maxiter\n",
    "    start[idx] = offset_sessions[maxiter]\n",
    "    end[idx] = offset_sessions[maxiter+1]\n",
    "print(start[:10])\n",
    "print(end[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23683\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>item_type</th>\n",
       "      <th>session</th>\n",
       "      <th>cross_session</th>\n",
       "      <th>item_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23697</th>\n",
       "      <td>52711942</td>\n",
       "      <td>1777612</td>\n",
       "      <td>1.314212e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>5636055</td>\n",
       "      <td>4562</td>\n",
       "      <td>3454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23698</th>\n",
       "      <td>52711942</td>\n",
       "      <td>2998253</td>\n",
       "      <td>1.314212e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>5636055</td>\n",
       "      <td>4562</td>\n",
       "      <td>8201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23699</th>\n",
       "      <td>52711942</td>\n",
       "      <td>1305435</td>\n",
       "      <td>1.314212e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>5636055</td>\n",
       "      <td>4562</td>\n",
       "      <td>8569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23700</th>\n",
       "      <td>52711942</td>\n",
       "      <td>1293318</td>\n",
       "      <td>1.314212e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>5636055</td>\n",
       "      <td>4562</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23701</th>\n",
       "      <td>52711942</td>\n",
       "      <td>1867314</td>\n",
       "      <td>1.314212e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>5636055</td>\n",
       "      <td>4562</td>\n",
       "      <td>8570</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id  item_id     timestamp  item_type  session  cross_session  \\\n",
       "23697  52711942  1777612  1.314212e+09          0  5636055           4562   \n",
       "23698  52711942  2998253  1.314212e+09          0  5636055           4562   \n",
       "23699  52711942  1305435  1.314212e+09          0  5636055           4562   \n",
       "23700  52711942  1293318  1.314212e+09          0  5636055           4562   \n",
       "23701  52711942  1867314  1.314212e+09          1  5636055           4562   \n",
       "\n",
       "       item_idx  \n",
       "23697      3454  \n",
       "23698      8201  \n",
       "23699      8569  \n",
       "23700        36  \n",
       "23701      8570  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finished = False\n",
    "endpoint_count = 0\n",
    "while not finished:\n",
    "    minlen = (end-start).min()\n",
    "    out_idx = data.item_idx.values[start]\n",
    "    for i in range(minlen-1):\n",
    "        in_idx = out_idx\n",
    "        out_idx = data.item_idx.values[start+i+1]\n",
    "        endpoint_count += len(out_idx)\n",
    "    \n",
    "    start = start+minlen-1\n",
    "    mask = np.arange(len(iters))[(end-start)<=1]\n",
    "\n",
    "    for idx in mask:\n",
    "        maxiter += 1\n",
    "        if maxiter >= len(offset_sessions)-1:\n",
    "            finished = True\n",
    "            break\n",
    "        iters[idx] = maxiter\n",
    "        start[idx] = offset_sessions[maxiter]\n",
    "        end[idx] = offset_sessions[maxiter+1]\n",
    "        \n",
    "print(max(start))\n",
    "data[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\tStep 1\tlr: 0.00100\tloss: 3.9121\tElapsed: 0.3\n",
      "Epoch 2\tStep 1000\tlr: 0.00100\tloss: 3.5198\tElapsed: 2.9\n",
      "1 epoch elapsed time: 3.476520299911499\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "tic = time.time()\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_cost = []\n",
    "    state = [np.zeros([batch_size, rnn_size], dtype=np.float32) for _ in range(layers)]\n",
    "    iters = np.arange(batch_size)\n",
    "    maxiter = iters.max()\n",
    "    \n",
    "    start = offset_sessions[iters]\n",
    "    end = offset_sessions[iters+1]\n",
    "    \n",
    "    finished = False\n",
    "    while not finished:\n",
    "        minlen = (end-start).min()\n",
    "        out_idx = data.item_idx.values[start]\n",
    "        for i in range(minlen-1):\n",
    "            in_idx = out_idx\n",
    "            out_idx = data.item_idx.values[start+i+1]\n",
    "            # prepare inputs, targeted outputs and hidden states\n",
    "            fetches = [cost, final_state, global_step, lr, train_op]\n",
    "            feed_dict = {X: in_idx, Y: out_idx}\n",
    "            for j in range(layers): \n",
    "                feed_dict[States[j]] = state[j]\n",
    "            \n",
    "            cost_, state, step, lr_, _ = sess.run(fetches, feed_dict)\n",
    "            epoch_cost.append(cost_)\n",
    "                \n",
    "            if step == 1 or step % print_step == 0:\n",
    "                avgc = np.mean(epoch_cost)\n",
    "                print('Epoch {}\\tStep {}\\tlr: {:.5f}\\tloss: {:.4f}\\tElapsed: {:.1f}'.\n",
    "                      format(epoch, step, lr_, avgc, time.time()-tic))\n",
    "\n",
    "        start = start+minlen-1\n",
    "        mask = np.arange(len(iters))[(end-start)<=1]\n",
    "        for idx in mask:\n",
    "            maxiter += 1\n",
    "            if maxiter >= len(offset_sessions)-1:\n",
    "                finished = True\n",
    "                break\n",
    "            iters[idx] = maxiter\n",
    "            start[idx] = offset_sessions[maxiter]\n",
    "            end[idx] = offset_sessions[maxiter+1]\n",
    "        if len(mask):\n",
    "            for i in range(layers):\n",
    "                state[i][mask] = 0\n",
    "        \n",
    "    avgc = np.mean(epoch_cost)\n",
    "    if np.isnan(avgc):\n",
    "        print('Epoch {}: Nan error!'.format(epoch, avgc))\n",
    "        break\n",
    "    saver.save(sess, '{}/gru-model'.format(checkpoint_dir), global_step=epoch)\n",
    "print(\"1 epoch elapsed time:\", time.time() - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parameters\n",
    "cut_off = 20     \n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\IWTL\\anaconda3\\envs\\env\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from .\\checkpoint\\gru-model-2\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "ckpt = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "saver.restore(sess, ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>item_type</th>\n",
       "      <th>session</th>\n",
       "      <th>cross_session</th>\n",
       "      <th>item_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3832130</td>\n",
       "      <td>1298038</td>\n",
       "      <td>1.314448e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>5643283</td>\n",
       "      <td>4563</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3832130</td>\n",
       "      <td>3184419</td>\n",
       "      <td>1.314448e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>5643283</td>\n",
       "      <td>4563</td>\n",
       "      <td>5853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36855984</td>\n",
       "      <td>1309206</td>\n",
       "      <td>1.314492e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>5644765</td>\n",
       "      <td>4564</td>\n",
       "      <td>8353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1180130</td>\n",
       "      <td>2159331</td>\n",
       "      <td>1.315044e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>5663490</td>\n",
       "      <td>4566</td>\n",
       "      <td>8460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4668061</td>\n",
       "      <td>3412830</td>\n",
       "      <td>1.315103e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>5665558</td>\n",
       "      <td>4567</td>\n",
       "      <td>7894</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id  item_id     timestamp  item_type  session  cross_session  \\\n",
       "0   3832130  1298038  1.314448e+09          0  5643283           4563   \n",
       "1   3832130  3184419  1.314448e+09          1  5643283           4563   \n",
       "2  36855984  1309206  1.314492e+09          0  5644765           4564   \n",
       "3   1180130  2159331  1.315044e+09          0  5663490           4566   \n",
       "4   4668061  3412830  1.315103e+09          0  5665558           4567   \n",
       "\n",
       "   item_idx  \n",
       "0        93  \n",
       "1      5853  \n",
       "2      8353  \n",
       "3      8460  \n",
       "4      7894  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## valdation data set\n",
    "valid['item_idx'] = valid['item_id'].map(lambda x: itemidmap[x])\n",
    "valid[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 3, 4, 6])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## valid offset sessions\n",
    "offset_sessions = np.zeros(valid['cross_session'].nunique()+1, dtype = np.int32)\n",
    "offset_sessions[1:] = valid.groupby('cross_session').size().cumsum()\n",
    "offset_sessions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init prediction\n",
    "if len(offset_sessions) - 1 < batch_size:\n",
    "    batch_size = len(offset_sessions) - 1\n",
    "\n",
    "iters = np.arange(batch_size).astype(np.int32)\n",
    "maxiter = iters.max()\n",
    "start = offset_sessions[iters]\n",
    "end = offset_sessions[iters+1]\n",
    "in_idx = np.zeros(batch_size, dtype=np.int32)\n",
    "predict_state = [np.zeros([batch_size, rnn_size], dtype=np.float32) for _ in range(layers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (32,) for Tensor 'input:0', which has shape '(50,)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-626bcd37b7d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mStates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_state\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mpreds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m### preds shape: (item_size, batch_size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1126\u001b[0m                              \u001b[1;34m'which has shape %r'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[1;32m-> 1128\u001b[1;33m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[0;32m   1129\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tensor %s may not be fed.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape (32,) for Tensor 'input:0', which has shape '(50,)'"
     ]
    }
   ],
   "source": [
    "evalutation_point_count = 0\n",
    "mrr, recall = 0.0, 0.0\n",
    "tic = time.time()\n",
    "while True:\n",
    "    valid_mask = iters >= 0\n",
    "    if valid_mask.sum() == 0:\n",
    "        print(\"break at endpoint\", evalutation_point_count)\n",
    "        break\n",
    "        \n",
    "    start_valid = start[valid_mask]\n",
    "    minlen = (end[valid_mask]-start_valid).min()\n",
    "    print(type(valid_mask))\n",
    "    in_idx[valid_mask] = valid.item_idx.values[start_valid]\n",
    "    \n",
    "    for i in range(minlen-1):\n",
    "        out_idx = valid.item_idx.values[start_valid+i+1]\n",
    "        ## --- prediction --- ##\n",
    "        fetches = [yhat_all, final_state]\n",
    "        feed_dict = {X: in_idx}\n",
    "        for j in range(layers): \n",
    "            feed_dict[States[j]] = predict_state[j]\n",
    "        preds, predict_state = sess.run(fetches, feed_dict)\n",
    "        preds = pd.DataFrame(data=np.asarray(preds).T)\n",
    "        preds.fillna(0, inplace=True) ### preds shape: (item_size, batch_size)\n",
    "        ## --- evaluation --- ##\n",
    "        in_idx[valid_mask] = out_idx\n",
    "        ### rank\n",
    "        ranks = (preds.values.T[valid_mask].T > \n",
    "                 np.diag(preds.loc[in_idx].values)[valid_mask]).sum(axis=0) + 1\n",
    "        ### cutoff->recall,mrr\n",
    "        rank_ok = ranks < cut_off\n",
    "        recall += rank_ok.sum()\n",
    "        mrr += (1.0 / ranks[rank_ok]).sum()\n",
    "        evalutation_point_count += len(ranks)\n",
    "        \n",
    "    start = start+minlen-1\n",
    "    mask = np.arange(len(iters))[(valid_mask) & (end-start<=1)]\n",
    "    \n",
    "    for idx in mask:\n",
    "        maxiter += 1\n",
    "        \n",
    "        if maxiter >= len(offset_sessions)-1:\n",
    "            iters[idx] = -1\n",
    "        else:\n",
    "            iters[idx] = maxiter\n",
    "            start[idx] = offset_sessions[maxiter]\n",
    "            end[idx] = offset_sessions[maxiter+1]\n",
    "            \n",
    "    if len(mask):\n",
    "        for i in range(layers):\n",
    "            predict_state[i][mask] = 0\n",
    "\n",
    "### metric\n",
    "recall = recall/evalutation_point_count\n",
    "mrr = mrr/evalutation_point_count\n",
    "print(\"recall: \", recall, \"mrr:\", mrr, \"elapsed time:\", time.time()-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval\n",
    "print(evalutation_point_count)\n",
    "print(sum(valid.groupby('cross_session').size() - 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
